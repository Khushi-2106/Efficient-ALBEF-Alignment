# &nbsp;ğŸ“‚ Dataset Setup for VQA and Reasoning (ALBEF-based Project)

# 

# All datasets are stored \*\*in Google Drive\*\*, not in this repository.

# Create a folder structure like:



MyDrive/data/

â”‚

â”œâ”€â”€ visual\_genome/

â”œâ”€â”€ coco/

â”œâ”€â”€ vqa/

â”œâ”€â”€ nlvr2/

â”œâ”€â”€ snli-ve/

â”œâ”€â”€ flickr30k/

â””â”€â”€ refcoco\_plus/





# 

# ---

# 

# \## 1ï¸âƒ£ Visual Genome (v1.2)

# 

# \*\*Purpose:\*\* region-level grounding and questionâ€“answer pretraining.

# 

# \*\*Download page:\*\* \[Visual Genome Downloads](https://visualgenome.org/api/v0/api\_home.html)

# 

# \*\*Download these files (v1.2):\*\*

# \- Images part 1 (VG\_100K) â€” 9.2 GB

# \- Images part 2 (VG\_100K\_2) â€” 5.47 GB

# \- image\_data.json â€” 17.6 MB

# \- region\_descriptions.json â€” 712 MB

# \- question\_answers.json â€” 803 MB

# \- objects.json â€” 413 MB

# \- attributes.json â€” 462 MB

# \- relationships.json â€” 709 MB

# 

# \*\*Skip:\*\* synsets, aliases, scene graphs.

# 

# \*\*Folder layout:\*\*



data/visual\_genome/

â”œâ”€â”€ images/VG\_100K/

â”œâ”€â”€ images/VG\_100K\_2/

â”œâ”€â”€ image\_data.json

â”œâ”€â”€ region\_descriptions.json

â”œâ”€â”€ question\_answers.json

â”œâ”€â”€ objects.json

â”œâ”€â”€ attributes.json

â””â”€â”€ relationships.json





---



\## 2ï¸âƒ£ COCO Captions (2014 or 2017)



\*\*Purpose:\*\* imageâ€“caption pretraining and retrieval.



\*\*Download:\*\* \[COCO Dataset](https://cocodataset.org/#download)



\- Images: `train2014`, `val2014` (â‰ˆ 20 GB)

\- Captions:

Â  - `captions\\\_train2014.json`

Â  - `captions\\\_val2014.json`

data/coco/

â”œâ”€â”€ train2014/

â”œâ”€â”€ val2014/

â”œâ”€â”€ captions\_train2014.json

â””â”€â”€ captions\_val2014.json





---



\## 3ï¸âƒ£ VQA v2.0



\*\*Purpose:\*\* fine-tuning and evaluating the Visual Question Answering task.



\*\*Download:\*\* \[VQA v2 Downloads](https://visualqa.org/download.html)



Files:

\- `v2\\\_Questions\\\_Train\\\_mscoco.zip`

\- `v2\\\_Questions\\\_Val\\\_mscoco.zip`

\- `v2\\\_Annotations\\\_Train\\\_mscoco.zip`

\- `v2\\\_Annotations\\\_Val\\\_mscoco.zip`



\*\*Uses the same COCO images.\*\*

â”œâ”€â”€ v2\_OpenEnded\_mscoco\_train2014\_questions.json

â”œâ”€â”€ v2\_mscoco\_train2014\_annotations.json

â”œâ”€â”€ v2\_OpenEnded\_mscoco\_val2014\_questions.json

â””â”€â”€ v2\_mscoco\_val2014\_annotations.json





---



\## 4ï¸âƒ£ NLVR2



\*\*Purpose:\*\* visual reasoning (two images + text).



\*\*Download:\*\* \[NLVR2 Dataset](https://lil.nlp.cornell.edu/nlvr/)



Files:

\- `train.zip`, `dev.zip`, `test.zip`

Extract images and JSON annotations.

data/nlvr2/

â”œâ”€â”€ images/

â”œâ”€â”€ train.json

â”œâ”€â”€ dev.json

â””â”€â”€ test.json





---



\## 5ï¸âƒ£ SNLI-VE



\*\*Purpose:\*\* visual-textual entailment.



\*\*Download:\*\* \[SNLI-VE GitHub](https://github.com/necla-ml/SNLI-VE)



Files:

\- `snli\\\_ve\\\_train.jsonl`

\- `snli\\\_ve\\\_dev.jsonl`

\- `snli\\\_ve\\\_test.jsonl`





---



\## 5ï¸âƒ£ SNLI-VE



\*\*Purpose:\*\* visual-textual entailment.



\*\*Download:\*\* \[SNLI-VE GitHub](https://github.com/necla-ml/SNLI-VE)



Files:

\- `snli\\\_ve\\\_train.jsonl`

\- `snli\\\_ve\\\_dev.jsonl`

\- `snli\\\_ve\\\_test.jsonl`

data/snli-ve/

â”œâ”€â”€ train.jsonl

â”œâ”€â”€ dev.jsonl

â””â”€â”€ test.jsonl





---



\## 6ï¸âƒ£ Flickr30k



\*\*Purpose:\*\* imageâ€“text retrieval and grounding evaluation.



\*\*Download:\*\* \[Flickr30k Entities](http://shannon.cs.illinois.edu/DenotationGraph/)



Files:

\- `flickr30k\\\_images/`

\- `flickr30k\\\_captions.json`

data/flickr30k/

â”œâ”€â”€ images/

â””â”€â”€ captions.json





---



\## 7ï¸âƒ£ RefCOCO+



\*\*Purpose:\*\* referring-expression / phrase-grounding evaluation.



\*\*Download:\*\* \[RefCOCO+ Repo](https://github.com/lichengunc/refer)



Files:

\- `refcoco+\\\_annotations.json`

\- COCO images (reuse from `data/coco`)



data/refcoco\_plus/

â”œâ”€â”€ annotations.json

â””â”€â”€ images/ â† link or copy to COCO images







---



\## âš™ï¸ Notes



\- Keep large datasets on Drive and mount Drive in Colab:

Â  ```python

Â  from google.colab import drive

Â  drive.mount('/content/drive')

Â  data\_root = "/content/drive/MyDrive/data"



